% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
%\renewcommand{\qedsymbol}{\filledbox}
 
\title{CS6700: Reinforcement Learning}%replace X with the appropriate number
\author{Varun Sundar, EE16B068\\ %replace with your name
 Submission - Assignment 1} %if necessary, replace with your course title
 
\maketitle
 
\section{Question : Computational Bounds}

We compute the cost of using both the DP algorithm and one that directly calculates $J_{\pi}$ for each policy $\pi$. \\

Notice that a policy maps the set of states $S$ to a set of actions $A$. WLOG assume both are discrete and finite. Clearly, given $n$ states and $m$ actions, the number of policies is $m^n$.\\

Further, assume that the process is markow  (if not, we club dependent states into a \textit{superstate}) and the argument repeats. Also, we shall work with non-stationary transition probabilities at each stage $k$, given by

$$P_{ij}^{a_k}$$
\quad \quad \quad \quad ...  where $a_k$ is the action taken at stage k. \\

Now, \\

\begin{align*}
 J_{\pi}(x_0)  & = E[\sum_{i=0}^{N-1}g(x_k,a_k,x_{k+1}) + g_N(x_N)]  \\ 
& = \sum_{k=0}^{N-1} \sum_{j=0}^{n} P_{x_k j}^{a_k} (g(x_k,a_k,j) + g_N(x_N)) & \text{where we substitute $x_{k+1}$ with $j$ } \\
& \text{for all states }  x_o .
\end{align*}



\quad \quad \quad \quad  .... computed for a given policy $\pi$, done for the entire set of policies. \\

Here, the computational cost is,

$$ O(Nn) +O(Nn) = O(Nn) $$

Hence, by computing the minimum over all such policies for every initial state we incur a cost $ O(Nn^2m^n) $.


Now, in the case of the DP algorithm, 

\begin{align*}
 J_{\pi}(x_k) & =  E[(g(x_k,a_k,x_{k+1}) + J_{\pi}(x_{k+1})]  \\
 & = \sum_{j=0}^{n}P_{ij}^{a_k} (g(x_k,a_k,x_{k+1}) + J_{\pi}(x_{k+1})) & \text{at a stage $k$ for state }x_k\\
\end{align*}
Hence, has the computational cost in terms of a recurrence, where

\begin{align*}
C_k=nm+m*C_{k+1}
\end{align*}

However, given a set of costs $J_k$ for each state $x_k$ we may store it in a lookup table. At each such stage, the computational cost is $O(nm)$.

Hence, the total computational cost is $O(Nn^2m)$ , calculated for all states and stages. Which is clearly much more efficient than the former $O(n^2M^n)$.

QED.

\section{Question 2: Exponential Cost}

a) We need to show that  that $J_k^∗$ are equal to the functions $J_k$ generated by the DP-like algorithm, at every stage $k$. 
For clarity,
 $ J_k(x_k)^*=min_{\pi_k, } E [ exp( g_N(x_N ) + \sum_{i=k}^{N-1}g_i(x_i,\pi(x_i),w_i)]$  
We do this by induction. (in the reverse order) \\

\textit{Base case} $J_N(x_N)=exp(g_N(x_N)) $ \text{by definition} \\

\textit{Assume Case k+1} \\
\textit{Then} \\

\begin{align*}
J_k(x_k)^*\\
&=min_{\mu_k,\pi_{k+1}} E_{w_k,...w_{N-1}} [ exp( g_N(x_N ) + \sum_{i=k}^{N-1}g_i(x_i,\pi(x_i),w_i)]\\
&= min_{\mu_k,\pi_{k+1}} E_{w_k,...w_{N-1}} [ exp(g_k(x_k,\pi(x_k),w_k))] E_{w_{k+1},...w_{N-1}}[exp(g_N(x_N ) + \sum_{i=k+1}^{N-1}g_i(x_i,\pi(x_i),w_i))] \\
&= min_{\mu_k} E_{w_k} [ exp(g_k(x_k,\pi(x_k),w_k))] min_{\pi_{k+1}} E_{w_{k+1},...w_{N-1}}[exp(g_N(x_N ) + \sum_{i=k+1}^{N-1}g_i(x_i,\pi(x_i),w_i))] \\
&= min_{\mu_k} E_{w_k} [ exp(g_k(x_k,\pi(x_k),w_k))*J_{k+1}(x_{k+1})^*]\\
&= min_{\mu_k} E_{w_k} [ exp(g_k(x_k,\pi(x_k),w_k))*J_{k+1}(x_{k+1})]\\
&=J_k(x_k)
\end{align*}
Where, we have used the \textit{Markow} assumption in the second equation.

b) We have $V_N(x_N) = g_N(x_N)$. We shall use $u_k$ to denote actions at stage k, and $w_k$ as the stochasticity in transition. 

Also, using the fact that $ln$ is monotone and hence directly applying to the DP algorithm in $J_k$, 

\begin{align*}
V_k(x_k)  \\
& = ln (min_{u_k} E_{w_k}[exp g_k(x_k, u_k) J_{k+1}(x_{k+1}) ]) \\
&=  min_{u_k} ln(E_{w_k}[exp g_k(x_k, u_k) J_{k+1}(x_{k+1}) ]) \\
& =min_{u_k} (g_k(x_k, u_k) + ln(E_{w_k}[J_{k+1}(x_{k+1}) ]) \\
& =min_{u_k} (g_k(x_k, u_k) + ln(E_{w_k}[exp(V_{k+1}(x_{k+1}) )])   \\
\end{align*}
\section{Question 3:  Food before(for?) movies}

a) Formulation:

\begin{itemize}
\item \textit{States}: 0,1  depicting sold or unsold.
\item  $p$ probability of a store having the food you like
\item \textit{Actions}: $a_{0}$ or $a_1$ to buy or not.
\item \textit{Terminal Costs: } $g_N(0)=1/(1-p) , g_N(1)=0 $ 
\item \textit{Transition Costs }: $g(x_k,a=a_0,T)=N-k$ , $g(x_k,a=a_1,x_{k+1})=0$
\end{itemize}

b) 
\begin{align*}
J_k(x_k)
& =min_{a \in A} E(g(x_k,a,x_{k+1})+J_{k+1}(x_{k+1})) \\
&=min E(J_{k+1}(x_{k+1})) , p(N-k)+(1-p) E(J_{k+1}(x_{k+1})) \\
&=min (J_{k+1}(1)) , p(N-k)+(1-p) (J_{k+1}(1)) \\
\end{align*}

if $x_k != T $ , $N-k$ otherwise.

Note that,

\begin{align}
&J_k(1)=pmin(N-k,J_{k+1}(1)) +J_{k+1}(1-p)  \\
& \implies J_k(1) \leqslant J_{k+1}(1) 
\end{align}
c) The solution is therefore a threshold given by,

Buy if $J_k(1) \geqslant N-k+1$ .
Else, move on.

To argue that if it is optimal to buy at store $k$ when the item you like is available, then it is optimal to buy at store $k + 1$ when the like-able food is available there, is equivalent to,

\begin{align*}
& N-k+1 \leqslant J_{k}(1) \implies N-k \leqslant J_{k+1}(1)\\
& \text{since}\\
& N-k  \leqslant N-k+1 \leqslant  J_{k}(1) \leqslant J_{k+1}(1)
\end{align*}
To show that E (Jk+1(x)) is a constant that depends on n − k, we turn towards $J_k(1)$. 

\subsection*{Claim}
$$ J_k(1)= (N-k) +(1-p)^{N-k-1} -(1-p)/p*(1-(1-p)^{N-k}) $$when $J_k(1)>N-k$
\subsection*{Proof}

Base case, $k=N$
$$ J_N(1) =1/(1-p) $$
Assume for $k+1$.

Then, 
\begin{align*}
J_k(1)
&=pmin(N-k,J_{k+1}(1))+(1-p)J_{k+1}(1) \\
& = p(N-k)+(1-p)((N-k-1) +(1-p)^{N-k-2} -(1-p)/p*(1-(1-p)^{N-k-1})\\
&=(N-k) +(p-1) +(1-p)^{N-k-1} -(1-p)/p*(1-p-(1-p)^{N-k})\\
&=  (N-k) +(1-p)^{N-k-1} -(1-p)/p*(1-(1-p)^{N-k})
\end{align*}
Hence proved.

Thus, suppose we are to buy at stage $k*$. Then it must not have been optimal before $k^*$ , and must be optimal at $k^*$.

\begin{align*}
& J_{k^*}(1) \geqslant N-k+1 \\
\implies  (1-p)^{N-k-1} -(1-p)/p*(1-(1-p)^{N-k}) \geqslant 1 \\
\implies  (1-p)^{N-k}  \geqslant (1-p)/(1-p+p^2) \\
\end{align*}

Hence the largest $k$ satisfying this (denoted as $k^*$) is the optimal stage to buy. Note that if $k^*$ > $N$ then do not buy.

\section{Question 4: Computer Job Scheduling}

Let a schedule be denoted as $\{ x_0,x_1,...,x_k,i,j,x_{k+1},...,x_N\}$, $T_i$ be the time taken by job $i$, to complete portion $\beta_i$ with a probability $p_i$, else the computer crashes with probability $(1-p_i)$. Let our two states of consideration be $T$ and $\overline{T} $ , where $T$ denotes that the computer has hung up. Also let $Z_i$ denote the residual time of job $i$. \\

Note that since a job may be rescheduled multiple times, we have $Z_i$ varying as $(1-\beta_i)^mT_i$ where $m$ is the number of times a particular job has been rescheduled. This directly follows by assuming scheduling probabilities for a given job are constant and the observation that a computer must not have crashed in each of these stages.

Let the reward function (reward transition function)

\[ r(x_k,x_{k+1}) = \begin{cases} 
          \beta_iZ_i & x_{k+1},x_k \neq  T \\
           0 & x_{k+1}=T , x_k \neq T\\
          0 & otherwise
       \end{cases}
    \]
    
    Total reward,
\begin{align*}
J_0(x_0) \\
& = E( \sum_{i=0}^{N-1} r(x_k,x_{k+1}) + r(x_N)) \\
& = E( \sum_{i=0}^{k} r(x_k,x_{k+1})+  r(i,j)+r(j,i)+ \sum_{i=k+1}^{N-1} r(x_k,x_{k+1}) + r(x_N))
\end{align*}
Now subtracting this for two schedules $s$ and $\overline{s}$ with $i$,$j$ interchanged,

\begin{align*}
s-\overline{s} \\
& = \eta (p_i\beta_iZ_j + p_ip_j\beta_jZ_j - p_j\beta_jZ_i + p_jp_i\beta_iZ_i) \\
& \text{ where $\eta $ represents the prior probabilities. } \\
& \geqslant 0 \\
& \implies p_i\beta_iZ_i(1-p_j) - p_j\beta_jZ_j(1-p_i) \geqslant 0 \\
& \implies \frac{p_i\beta_iZ_i}{1-p_i} - \frac{p_j\beta_jZ_j}{1-p_j} \geqslant 0 
\end{align*}

Which can be used to order the given jobs. (schedule in descending order).

\section{Question 5: Monotonicity}

a) We shall prove this by induction on the number of stages left, $k$ (reverse order).
Given, $ J_{N−1}(x) ≤ J_N(x)$ for all $x ∈ S$
Assume $J_{k+1}(x) \leqslant J_{k+2}(x) $ for all states $x$.
From the stationary cost and time invariant states and actions , we may write,

\begin{align*}
J_k(x)= \\
& min_{u \in U} E_{w}{g(x,u,w)+J_{k+1}(f(x,u,w)}. \\
& \leqslant min_{u \in U} E_{w}{g(x,u,w)+J_{k+2}(f(x,u,w))} \\
& \leqslant J_{k+1}(x) \\
& \text{where } f(x,u,w) \text{denotes the next state.}
\end{align*}
QED.

b) Follows a similar proof as a).
\\
For completeness,
We shall prove this by induction on the number of stages left, $k$ (reverse order).
Given, $ J_{N−1}(x) \geqslant J_N(x)$ for all $x ∈ S$
Assume $J_{k+1}(x) \geqslant J_{k+2}(x) $ for all states $x$.
From the stationary cost and time invariant states and actions , we may write,

\begin{align*}
J_k(x)= \\
& min_{u \in U} E_{w}{g(x,u,w)+J_{k+1}(f(x,u,w)}. \\
& \geqslant min_{u \in U} E_{w}{g(x,u,w)+J_{k+2}(f(x,u,w))} \\
& \geqslant J_{k+1}(x) \\
& \text{where } f(x,u,w) \text{denotes the next state.}
\end{align*}
QED.

\section{Question 6: Course notes proofreading}

a) Formulation:

\begin{itemize}
\item  Stages: number of students $N$.
\item States: number of errors left at every stage $\{x_0,..x_k,...x_{N-1},x_{N}\}$ and a terminal state T . Note that the last state is kept different from the terminal state to account for intermittent stopping. Also, note that $x_0$ is $X$.
\item Note that $x_k \geqslant x_{k+1}$ .
\item Actions: $a, \overline{a}$ (continue to next student or stop)
\item Cost functions:
\end{itemize}

\[ g(x_k,a_k,x_{k+1}) = \begin{cases} 
          c_1,x_k \neq  T, a_k=a \\
           c_2x_k & x_k \neq  T, a_k=\overline{a}\\
          0 & otherwise
       \end{cases}
    \]
   \[ g_N(x_N) = \begin{cases} 
           c_2x_N & x_N \neq  T \\
          0 & otherwise
       \end{cases}
    \]
   b,c) Now the optimal cost to go at stage $k$ , \\
   
   \begin{align*}
J_k(x_k)^* \\
& = min_{a,\overline{a}}E( g(x_k,a_k,x_{k+1}) + J_{k+1}(x_{k+1})) \\
& = min \{ E( g(x_k,a,x_{k+1}) + J_{k+1}(x_{k+1}) ), c_2x_k \} \\
&= min \{ c_1 +E[ J_{k+1}(x_{k+1})], c_2x_k \}
\end{align*}

 So continue to proof read if,
 
 \begin{align*}
 & c_1 + E(J _{k+1}(x_{k+1}) )< c_2x_k \\
 & \implies c_1 + \sum_{i=0}^{x_k} \binom{x_k}{i} (1-p)^i p^{x_k-i} J_{k+1}(i)  < c_2 x_k\\
 & \text{where we assume a binomial distribution of the number of errors picked by a student.} \\
 & \text{Combining the terms of} x_k \text{, we get if ...} \\
 & \alpha_k > c_1  \text{ then continue} 
 \end{align*}
 Note that if it is optimal to stop proof reading at $k$, then,
  \begin{align*}
 & c_1 + E(J _{k+1}(x_{k+1}) )>c_2x_k>c_2x_{k+1} \\
 & \implies c_1 + \sum_{i=0}^{x_k} \binom{x_k}{i} (1-p)^i p^{x_k-i} J_{k+1}(i)  > c_2 x_k\\
 \end{align*}
 However, we cannot comment on the monotonicity, since it is possible that the none of the students till the last 
 Hence, we obtain a threshold based policy at a given stage $k$. With, threshold $\alpha_k$ at stage $k$. \\
 
 \begin{align*}
 \alpha_k= c_2x_k - \sum_{i=0}^{x_k} \binom{x_k}{i} (1-p)^i p^{x_k-i} J_{k+1}(i)  \geqslant c_1 
 \end{align*}
 
 d) For a random $X$, modification, (this however turns the problem into a non-determinate number of states)
 
\begin{itemize}
\item  Stages: number of students $N$.
\item States: number of errors detected till stage  $k$ $\{x_0,..x_k,...x_{N-1},x_{N}\}$ and a terminal state T . Note that the last state is kept different from the terminal state to account for intermittent stopping. Also, note that $x_0$ is $0$.
\item Note that $x_k \leqslant x_{k+1}$ .
\item Actions: $a, \overline{a}$ (continue to next student or stop)
\item Terminal state reached when stopped or $x_k \geqslant X$.
\item  Cost functions:
\end{itemize}

\[ g(x_k,a_k,x_{k+1}) = \begin{cases} 
          c_1, & x_k \neq  T, a_k=a , x_k < X\\
           c_2(X-x_k) & x_k \neq  T, a_k=\overline{a}, x_k < X\\
          0 & otherwise
       \end{cases}
    \]
    \[ g_N(x_N) = \begin{cases} 
           c_2(X -x_N ) & x_N \neq  T \\
          0 & otherwise
       \end{cases}
    \]
    \begin{align*}
J_k(x_0)^* \\
& = min_{a,\overline{a}}E( g(x_k,a_k,x_{k+1}) + J_{k+1}(x_{k+1})) \\
& = min \{ E( g(x_k,a,x_{k+1}) + J_{k+1}(x_{k+1}) ), c_2x_k \} \\
&= min \{ c_1 +E[ J_{k+1}(x_{k+1})]), c_2x_k \}
\end{align*}
    Issue with this formulation: specifying the expectation is not straightforward.
 
 \section{References}

This submission comprises of genuine work by the student, with the following references attributed below:

\begin{itemize}
\item Question 1: NA.
\item Question 2: Class notes and Bertsekas Ch 1.
\item Question 3: Bertsekas Ch 4. (similar to 4.19)
\item Question 4: Index based problems discussed in class. Motivation for part (c) was discussed with classmate - Siddharth Nayak (EE16B073).
\item Question 5: Bertsekas Ch 1,2.
\item Question 6: Asset selling problem discussed in class.
\end{itemize}
    
\end{document}