BETA 0.0
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 16.   15.    4.5]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 16.   15.    4.5]
Final Optimal cost associated with the policy
[ 16.   15.    4.5]
BETA 0.05
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 16.54406586  15.75739423   5.21856841]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 16.54406197  15.75738985   5.21856449]
Final Optimal cost associated with the policy
[ 16.54406586  15.75739423   5.21856841]
BETA 0.1
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 17.15806025  16.59699455   6.0031447 ]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 17.1579291   16.59684672   6.00301243]
Final Optimal cost associated with the policy
[ 17.15806025  16.59699455   6.0031447 ]
BETA 0.15
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 17.85400443  17.53295028   6.8660547 ]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 17.85294964  17.53176269   6.86499109]
Final Optimal cost associated with the policy
[ 17.85400443  17.53295028   6.8660547 ]
BETA 0.2
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 18.64696457  18.58287342   7.82268115]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 18.64224053  18.57756127   7.81791854]
Final Optimal cost associated with the policy
[ 18.64696457  18.58287342   7.82268115]
BETA 0.25
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 19.55606981  19.76897117   8.89248867]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 19.54068732  19.75169739   8.87698361]
Final Optimal cost associated with the policy
[ 19.55606981  19.76897117   8.89248867]
BETA 0.3
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 20.60596865  21.1196557   10.100489  ]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 20.56494423  21.07365745  10.05914591]
Final Optimal cost associated with the policy
[ 20.60596865  21.1196557   10.100489  ]
BETA 0.35
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 21.82896027  22.67188473  11.47938413]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 21.73343411  22.56495928  11.38313582]
Final Optimal cost associated with the policy
[ 21.82896027  22.67188473  11.47938413]
BETA 0.4
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 23.26819598  24.47465307  13.07278391]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 23.06634844  24.24915215  12.86945449]
Final Optimal cost associated with the policy
[ 23.26819598  24.47465307  13.07278391]
BETA 0.45
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 24.98263383  26.59435749  14.94018318]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 24.58564736  26.15181675  14.54037387]
Final Optimal cost associated with the policy
[ 24.98263383  26.59435749  14.94018318]
BETA 0.5
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 27.05497771  29.12332838  17.16493314]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 26.31505966  28.30056524  16.41993666]
Final Optimal cost associated with the policy
[ 27.05497771  29.12332838  17.16493314]
BETA 0.55
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 29.60492927  32.19395886  19.86754081]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 28.28008276  30.72504121  18.53395634]
Final Optimal cost associated with the policy
[ 29.60492927  32.19395886  19.86754081]
BETA 0.6
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 32.81241492  36.00326708  23.22896815]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 30.50798271  33.45691967  20.91001718]
Final Optimal cost associated with the policy
[ 32.81241492  36.00326708  23.22896815]
BETA 0.65
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 36.96079076  40.85820022  27.53395004]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 33.02779424  36.52990709  23.5774742 ]
Final Optimal cost associated with the policy
[ 36.96079076  40.85820022  27.53395004]
BETA 0.7
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 42.52339205  47.26561034  33.25772809]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 35.8703207   39.97974138  26.56745322]
Final Optimal cost associated with the policy
[ 42.52339205  47.26561034  33.25772809]
BETA 0.75
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 50.35424764  56.12881732  41.2570794 ]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 39.06813407  43.84419188  29.91285083]
Final Optimal cost associated with the policy
[ 50.35424764  56.12881732  41.2570794 ]
BETA 0.8
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 62.1656051   69.23566879  53.24840764]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 42.655575    48.16305938  33.64833438]
Final Optimal cost associated with the policy
[ 62.1656051   69.23566879  53.24840764]
BETA 0.85
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 81.96255404  90.713441    73.24345547]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 46.66875277  52.97817609  37.810342  ]
Final Optimal cost associated with the policy
[ 81.96255404  90.713441    73.24345547]
BETA 0.9
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 121.78851002  132.8167706   113.2970678 ]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 51.14554531  58.33340568  42.43708262]
Final Optimal cost associated with the policy
[ 121.78851002  132.8167706   113.2970678 ]
BETA 0.95
Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final optimal cost
[ 241.97938806  256.28102819  233.76669206]


 ------------ 


Modified Policy Iteration: Starting with initial policy [0 0 0]
Final optimal policy
{'state 0': 'action 1', 'state 1': 'action 1', 'state 2': 'action 2'}
Final MPI cost
[ 56.12559918  64.27464326  47.56853591]
Final Optimal cost associated with the policy
[ 241.97938806  256.28102819  233.76669206]
